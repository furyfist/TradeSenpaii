# TradeSenpai: A Full Project Retrospective
### From First Idea to Version 3 — A Complete Development Narrative

---

## Preface

This document is a comprehensive retrospective of TradeSenpai, an AI-powered stock research tool built from scratch across three major versions. It covers the full arc of the project: the original idea, the assumptions we started with, every major technical decision and why it was made, the bugs and failures we hit, the pivots and redesigns, the current state, and the lessons learned. Nothing is glossed over. This is an honest account of what it actually looks like to build something real from zero.

---

## Part 1: The Original Idea and What Problem It Was Trying to Solve

The starting premise was simple and genuine: retail investors are at a structural disadvantage compared to institutional analysts. Not because markets are rigged, but because the information asymmetry is real. Institutional desks have teams of analysts reading SEC filings, building sentiment models, running technical screens, and synthesizing everything into actionable briefs. A retail investor sitting at home has Yahoo Finance, Reddit, and gut instinct.

The idea behind TradeSenpai was to close a small part of that gap. Not to predict stock prices — that was explicitly ruled out from the beginning — but to surface the same kinds of signals that institutional-grade research surfaces: SEC filing sentiment, technical regime indicators, historical pattern matching, and model-based directional probability. The goal was educational. The tagline was always: "See what institutional analysts see."

The choice to focus on SEC filings as a core signal source was deliberate. Most retail-facing tools ignore filings entirely or reduce them to earnings headlines. But 10-Ks, 10-Qs, and 8-Ks contain language that moves markets. A company increasing its use of words like "uncertain," "litigation," "constrain," and "adverse" in its MD&A section is telling you something before the headline writers do.

The project was never meant to be a trading system. It was meant to be a research augmentation tool — something that does the grunt work of reading filings and computing technical features so a person can focus on the actual thinking.

---

## Part 2: The First Assumptions and Where They Came From

At the outset, several assumptions were baked into the design:

**Assumption 1: FinBERT would be the right sentiment model.**
The initial instinct was to use FinBERT, a BERT-based model fine-tuned on financial text. It was the obvious choice — well-known, widely used in academic finance NLP research, and pre-trained on financial data. This assumption turned out to be wrong.

**Assumption 2: Modern deep learning would beat classical methods on financial text.**
This was the mainstream assumption in ML circles: more parameters, more context, better results. Again, wrong for this specific domain.

**Assumption 3: High CV accuracy was the goal.**
Early in development there was implicit pressure to push model accuracy higher — to 55%, 60%, maybe 65%. This was a mistake in framing. In financial time series prediction, 52% cross-validation accuracy on a binary direction classification task is genuinely meaningful, not a failure. The baseline is a coin flip. The project eventually embraced this honestly, which was a better intellectual position.

**Assumption 4: The architecture could be general-purpose.**
Early thinking was to build one model that works across many stocks. The project eventually moved to per-ticker models — one transformer per stock — which was the right call.

**Assumption 5: The frontend would be simple.**
This assumption held for V1 but became increasingly wrong through V2 and V3 as the UI grew from a single page to a multi-route application with complex state management.

---

## Part 3: Version 1 — Proof of Concept and the FinBERT Mistake

Version 1 was a proof of concept. The scope was narrow: build a pipeline that could fetch SEC filings for a single ticker (KO, Coca-Cola), compute sentiment scores, combine them with price features, and train a model to predict next-day direction.

### The SEC Filing Pipeline

The first engineering challenge was getting SEC data reliably. The EDGAR API is public but underdocumented. Getting the filings index for a specific CIK (Central Index Key), fetching actual filing documents, and parsing the useful text out of the HTML and XBRL soup was non-trivial. Each filing is a different length. Some are 200 pages. Some are formatted with nested tables. Preprocessing had to be aggressive — strip all HTML, remove boilerplate legal language, handle encoding issues.

The output of this pipeline was a CSV per ticker containing one row per filing with cleaned text. This became `sec_sentiment_input.csv`, which remained a core data artifact through all three versions.

### The FinBERT Failure

FinBERT was integrated first. It produced sentiment scores — positive, negative, neutral probabilities — for each filing. The scores looked plausible. KO filings generally scored positive. JNJ filings during litigation periods scored negative. But when these scores were fed into the model, they added noise rather than signal.

The problem, identified after significant debugging, was fundamental: FinBERT was trained on financial news, analyst reports, and earnings call transcripts. SEC filings are written in a completely different register. Legal language, regulatory hedging, and mandatory disclosure boilerplate create a semantic distribution that FinBERT was never exposed to during training. A phrase like "the Company does not currently anticipate material adverse effects" is neutral-to-positive in SEC filing language but would score negatively under FinBERT's learned associations.

### The Loughran-McDonald Discovery

The pivot to the Loughran-McDonald (LM) dictionary was one of the most consequential decisions in the entire project. The LM dictionary is a curated wordlist developed specifically for financial document analysis, accounting for the domain-specific meanings that standard sentiment lexicons miss. In finance, words like "liability," "risk," "uncertainty," and "decline" have specific technical meanings rather than purely negative connotations.

The LM dictionary produces six category counts per document: positive, negative, uncertain, litigious, constraining, and superfluous. From these counts, normalized percentages and derived features like sentiment score, sentiment delta, uncertainty z-score, and litigation spike flag were engineered.

The result was dramatic. LM-based features outperformed FinBERT on the SEC filing task. This validated the core principle that was later written explicitly into the project's design guidelines: **domain-specific beats general-purpose**. An interpretable lexicon built for the exact document type outperformed a large pre-trained neural network. This lesson extended well beyond this specific project.

### The Model Choice: Why a Transformer

The model architecture chosen was a Transformer encoder with a classification head. The choice was driven by the sequential nature of the input: the model receives a 60-day lookback window of feature vectors (56 features per day) and predicts the next day's direction.

An LSTM or GRU would have been a reasonable alternative. The Transformer was chosen because of its ability to attend to non-local patterns in the sequence — a sentiment spike 45 days ago might be more relevant than what happened 3 days ago, and attention handles this more naturally than recurrent architectures. The specific configuration settled on was: d_model=128, 4 attention heads, 3 layers, dropout=0.3, pre-norm (norm_first=True), with a classifier head Linear(128→64) → GELU → Dropout → Linear(64→2).

Pre-norm (applying layer normalization before the attention and feedforward sub-layers rather than after) was specifically chosen for training stability. Post-norm transformers are known to require careful learning rate warmup; pre-norm is more robust.

### V1 Results

Version 1 produced a working single-ticker pipeline for KO with approximately 52% cross-validation accuracy. This was considered a success given the difficulty of the task, not a failure. The model was saved as a PyTorch checkpoint with all necessary metadata embedded: model config, feature columns, scaler parameters, CV accuracy, and training date.

---

## Part 4: Version 2 — Scaling to Six Tickers and the Agentic Backend

Version 2 was a significant expansion. The goal was to make the tool genuinely useful: support six tickers (KO, JNJ, PG, WMT, AAPL, GOOGL), build a real FastAPI backend, build a React frontend, and add an explanation layer using LLM synthesis.

### Why These Six Tickers

The ticker selection was deliberate. Consumer staples (KO, PG), healthcare (JNJ), retail (WMT), and technology (AAPL, GOOGL) were chosen to cover different sectors with different volatility profiles and sentiment dynamics. They are also large-cap, liquid stocks with long SEC filing histories — important for training data depth.

All six tickers have CIKs on EDGAR, active 10-K/10-Q filing schedules, and sufficient price history for the 200-day moving average warmup that the feature engineering pipeline requires.

### The Multi-Ticker Bug That Cost Hours

The most painful bug in V2 development was a hardcoded ticker reference. When building the multi-ticker pipeline, a function that was supposed to accept a ticker parameter had "KO" hardcoded in one location. The result was that running the pipeline for any ticker — JNJ, AAPL, GOOGL — silently fetched KO's price data. Every model trained on the "wrong" data.

This bug was invisible because the code ran without errors. The models trained, produced accuracy numbers, and saved checkpoints. The numbers looked wrong but not obviously wrong. It took hours of debugging to trace the issue to a single hardcoded string in the price fetching function.

This incident directly produced a rule that was written into the project's documented principles: **always log the ticker in every print statement, always pass ticker as a parameter, never hardcode ticker symbols**. This rule was enforced in every subsequent function:

```python
print(f"[INFO] Fetching {ticker} prices...")
```

A simple discipline that makes multi-ticker bugs immediately visible.

### The 56-Feature Engineering Pipeline

Version 2 formalized the feature set into 56 features per trading day, organized into semantic groups:

- **Price features**: daily return, gap percentage, lagged closes (1, 5, 10 days), moving averages (7, 20, 50, 200 days), volatility (20, 30 days), volume features, momentum (5d, 10d), Bollinger band distances, RSI(14)
- **Calendar features**: day of week, month, quarter
- **Regime feature**: binary above/below MA200 (market regime encoding)
- **LM base counts**: positive, negative, uncertain, litigious, constraining word counts
- **LM normalized**: percentage forms of the above
- **LM derived**: sentiment score, 5/20-day moving averages of sentiment, sentiment delta, uncertainty z-score, litigation spike flag, negative dominant flag
- **Sentiment lags**: 1, 5, 10-day lags of key sentiment features
- **Return lags**: 1, 2, 3, 5-day return lags
- **Regime flags**: vol_regime, RSI oversold/overbought, MA crossover flags, volume surge
- **Interaction terms**: sentiment × volatility, sentiment × uncertainty

The 200-day moving average created a data warmup problem. To compute MA200 for the first valid row, you need 200 prior days of price data. Combined with the 60-day sequence length for the transformer, you need at minimum 260 rows before getting a single usable training example. The solution was to always fetch `days=800` from yfinance — providing enough buffer for warmup, sequence construction, and validation split.

Failing to account for this warmup was a recurring source of subtle bugs: models would train on shorter-than-expected datasets, feature rows with NaN values would silently corrupt training batches, or the scaler would fit on partial data.

### The dropna() Problem

Related to the warmup issue was the `dropna()` problem. After computing rolling features (moving averages, volatility, momentum), many early rows contain NaN values. `dropna()` removes them. The temptation was to call `dropna()` early in the pipeline. But calling it before the rolling computation was complete would delete rows that later became valid; calling it after would delete rows whose NaN was legitimate and should have been imputed.

The discipline enforced was: compute all rolling features first, then apply `dropna()` once at the end, and always verify the resulting DataFrame length is sensible.

### The torch.load PyTorch 2.6 Breaking Change

A significant breaking change arrived with PyTorch 2.6: the default behavior of `torch.load()` changed to `weights_only=True`. This broke model loading for any checkpoint that contained non-tensor objects — which all TradeSenpai checkpoints did (they embedded Python dicts with config metadata, scaler parameters, feature column lists, etc.).

The fix was one keyword argument:
```python
torch.load(path, map_location="cpu", weights_only=False)
```

But finding this was not obvious. The error message from PyTorch was unhelpful, and the fix was buried in release notes. This became a documented rule: **always include `weights_only=False` when loading TradeSenpai model checkpoints**.

### The Explainer Pipeline and Groq Integration

V2 introduced an explanation layer — the `/explain` endpoint. The pipeline worked as follows:

1. **Similarity search**: cosine similarity over 38 features to find the top 3 most similar historical trading days from the full merged dataset, filtered to exclude the last 365 days (analogies must be genuinely historical)
2. **Web search enrichment**: for each historical analogy, Groq's compound search model (`groq/compound`) fetches what actually happened on that date — earnings results, analyst actions, macro events
3. **LLM synthesis**: `llama-3.3-70b-versatile` receives all context and generates a structured JSON explanation

The model name issue with Groq was a real stumbling block. The correct model identifier for Groq's search model was `"groq/compound"` — not `"compound-beta"`, not `"groq-compound"`. This distinction matters because Groq's API returns a 404 for incorrect model names, which is easy to mistake for a network or authentication error.

### The Hardcoded API Key Technical Debt

In V2, the Groq API key was hardcoded directly in `explainer.py`. This was acknowledged as technical debt at the time but not fixed immediately. It was eventually corrected in V3 when `python-dotenv` was introduced and all API keys moved to `.env`. The lesson here is straightforward: do not hardcode credentials ever, even temporarily. "Temporary" technical debt has a way of persisting.

### V2 Frontend Architecture

The V2 frontend was a single-page React application built with Vite. The key architectural decision was the loading pattern: fast endpoints (price history, sentiment history, model info) loaded in parallel via `Promise.all`, which allowed the chart and gauge components to render immediately while the slower prediction endpoint loaded separately. This gave the UI a responsive feel without a full loading spinner blocking everything.

The `ExplanationPanel` component introduced a state reset bug: when switching tickers, the explanation from the previous ticker would persist until the new one loaded. The fix was a `useEffect` hook watching the `ticker` prop that reset the explanation state on every ticker change. This was documented as a lesson: **always reset UI state on context change**.

All styling in V2 was inline CSS — no CSS framework, no CSS modules, no styled-components. This was intentional for simplicity but created increasingly verbose component files. In V3, the aesthetic was formalized into a consistent design system (IBM Plex Mono, dark terminal aesthetic, amber/gold accents) even while remaining inline.

---

## Part 5: Version 3 — The Research Platform

V3 was the largest architectural leap. Two major features were added: the Hypothesis Engine (a 6-agent AI research pipeline) and the Telegram Alert System (a scheduled notification service with subscriber management).

### Feature 1: The Hypothesis Engine

The Hypothesis Engine was the most ambitious feature in the project. The concept: a user types a natural language market hypothesis ("Coca-Cola will reach $300 in 3 months"), and 6 AI agents independently research, validate, and stress-test it before synthesizing a structured research brief.

#### Agent 1: Hypothesis Parser

The first challenge was parsing unstructured text into structured data. The parser needed to:
- Identify the ticker from company names, abbreviations, or dollar-sign notation
- Extract a target price if mentioned
- Extract a timeframe and convert it to days
- Calculate implied return percentage
- Classify the hypothesis type
- Flag statistically unrealistic hypotheses

The ticker matching used three-pass logic: dollar-sign notation (`$AAPL`), uppercase word match (`AAPL`), and company name alias match ("Apple", "Coca-Cola", "J&J"). The alias dictionary was essential — users type company names, not tickers.

The most subtle component was the realism check. For a hypothesis like "Coca-Cola will reach $300 in 3 months" (KO trading at ~$80, implying a 272% return in 90 days), the parser fetches 5 years of historical data, computes the rolling 90-day return distribution, and calculates a z-score. KO's 90-day return standard deviation is approximately 7.6%. A 272% implied return produces a z-score of 35.75 — unmistakably unrealistic, flagged immediately.

**The parser naming collision bug**: The file was originally named `parser.py`. Python has a standard library module also called `parser`. When running `python parser.py`, Python would import the stdlib module instead of the local file, causing a silent failure with no output and no error. The fix was renaming to `hypothesis_parser.py`. This is the kind of bug that takes longer to diagnose than to fix.

#### Agent 2: Market Context Collector

This agent reused the existing `feature_engineer.py` infrastructure — specifically `get_latest_feature_row()` — to pull the current technical state for the identified ticker. The key insight was that all the infrastructure for computing live technical signals already existed in the V2 backend. Agent 2 was essentially a structured wrapper around it.

The path resolution issue reappeared here. `sentiment_loader.py` used `BASE_PATH = Path("../../stock-analysis/data/processed")` — a relative path that worked when running from the `backend/` directory but broke when called from the `hypothesis/` subdirectory. The fix was standardizing all path resolution to use `Path(__file__).resolve().parent...` — file-relative rather than working-directory-relative.

This same bug appeared in `similarity_search.py`. It was the third time in the project that relative path resolution had caused a runtime failure. The lesson was eventually codified: **never use relative paths in Python files that may be imported from different working directories. Always use `__file__` as the anchor.**

The percentage scaling bug was another find: `distance_from_ma20`, `momentum_5d`, and `volatility_20` were already stored as percentages in the feature set (e.g., 3.23 meaning 3.23%), but the code was multiplying by 100 again, producing values like 323.15%. Caught by eyeballing the output — KO's distance from MA20 cannot be 323%.

#### Agent 3: Historical Evidence

This agent computed base rates from the merged dataset: what percentage of historical 90-day windows produced returns equal to or exceeding the implied move? The core calculation was a rolling forward-return series over the full price history, then simple percentage counts against various thresholds.

The `find_similar_days()` function from `similarity_search.py` was reused here for finding the top 3 most similar historical setups. The initial call was missing the required `current_features` parameter — the function signature had changed from V2 but the call site wasn't updated. The fix required fetching the latest feature row first and passing it explicitly.

The base rate outputs were genuinely informative: for the "KO reaches $300" hypothesis, `base_rate_for_implied` was 0.0% — the implied move had literally never occurred in the full 7,799-period historical record. For the more realistic "AAPL drops 8.84% in 180 days" hypothesis, base rate was 20.44% — possible but not the default outcome.

#### Agents 4 and 5: Bear Case and Bull Case

The original design used `groq/compound` for both bear and bull case research. This failed with a 413 Request Entity Too Large error — the compound model has a significantly smaller input token limit than standard models. Even after reducing max_tokens and tightening the system prompt, the error persisted.

The first attempted fix (two-step: compound for search, llama for formatting) also failed with 413. The compound model itself was rejecting requests that exceeded its input budget.

The pivot to **Tavily** was a clean solution. Tavily is a search API purpose-built for AI applications — it returns structured search results (title, URL, content snippet) rather than raw HTML, making it ideal for feeding into a formatting LLM. The two-step architecture became: Tavily fetches 3 relevant results → llama-3.3-70b-versatile formats them into a structured JSON array.

This pipeline produced high-quality, sourced, current-data outputs. For KO, the bear case correctly identified currency headwinds (5-point EPS drag in 2025), growth uncertainty (4-5% guidance that may not impress), and sector headwinds. Source URLs pointed to real articles from Nasdaq, GuruFocus, and 247WallSt — published in 2026, not stale training data.

#### Agent 6: Synthesizer

The synthesizer was the most prompt-engineering-intensive component. It received all five agent outputs and needed to produce a coherent, structured research brief. The first version truncated because `max_tokens=1200` was insufficient for the full JSON response. Increasing to `max_tokens=2000` resolved the truncation.

The `_parse_json()` function needed to be robust to partially truncated JSON — the bracket-closing salvage logic (counting unclosed `{` and `[` characters and appending the appropriate closers) was a practical fix for a real failure mode.

The `feasibility_score` computation (0-100) was deliberately designed to be interpretable:
- 40 points from base rate for the implied move
- 30 points from technical alignment (regime, MA crossover, momentum direction)
- 30 points from the realism check (penalizes high z-scores)

A score of 90 for "KO reaches $90 in 3 months" (11.84% implied return, 20% base rate, bullish regime) felt correct. A score near zero for "KO reaches $300 in 3 months" also felt correct.

### The POST /hypothesis Endpoint

Wiring the 6-agent pipeline into FastAPI was straightforward. The endpoint parses the hypothesis, runs all 5 data agents sequentially, synthesizes, and returns the brief. The only complexity was error handling: if Agent 1 fails to identify a ticker, there's no point running Agents 2-6. The parser's `error` field is checked first and returns a 400 immediately if set.

The `__init__.py` file in the hypothesis folder was required for Python to treat it as a package for imports from `main.py`. Forgetting this caused an `ImportError` on startup — another small thing that isn't obvious until you hit it.

### Feature 2: The Telegram Alert System

The Telegram Alert System was conceptually simpler than the Hypothesis Engine but had more moving parts to coordinate.

#### Architecture

Five files were built:

1. **`telegram_bot.py`**: The send wrapper. Async Telegram bot wrapped in `asyncio.run()` for synchronous compatibility with APScheduler jobs.

2. **`alert_store.py`**: SQLite persistence layer for three tables: `sent_alerts` (deduplication), `subscribers` (user management), `prediction_history` (accuracy tracking).

3. **`watcher.py`**: Signal monitors for direction flips, sentiment spikes, and litigation flag changes.

4. **`digest.py`**: Message formatters for each alert type (morning brief, evening brief, weekly digest, per-event alerts).

5. **`scheduler.py`**: APScheduler background scheduler wired into FastAPI's lifespan event.

#### The APScheduler Integration

Integrating APScheduler into FastAPI's lifespan context was clean but required care with the background thread for the Telegram bot listener. The bot listener (for auto-approving subscribers) runs as a daemon thread using `asyncio.run()` in a separate thread from the main FastAPI event loop. Using `daemon=True` ensures it doesn't block server shutdown.

#### The Subscriber System and Chat ID Problem

The initial subscriber flow required manual admin intervention: user submits username → admin fetches their chat ID from `getUpdates` → admin approves via `/admin` panel. This was functional but operationally inconvenient.

The automated solution used a bot listener (`bot_listener.py`) that polls for `/start` commands. When a user sends `/start` to the bot, the listener looks up their Telegram username in the pending subscribers table. If found, it auto-approves with their chat ID (obtained from the message itself) and sends a welcome message. No admin action required.

This is only possible because Telegram message objects contain both the username and the chat ID. The username is the lookup key (matching the website submission), and the chat ID is extracted from the message for storage.

#### The `broadcast()` Function

A critical oversight in the initial implementation: all scheduled jobs called `send_message()`, which sent only to the admin's chat ID from `.env`. The multi-user subscriber system was built but not actually wired into the scheduler.

The fix was renaming: `broadcast()` in `telegram_bot.py` fetches all approved subscriber chat IDs from SQLite and sends to each one, plus the admin. The scheduler import was changed from `send_message` to `broadcast as send_message` — a one-line alias that made every scheduler job automatically broadcast without changing any other code.

#### Prediction History Logging

The evening brief originally hardcoded accuracy at 52%. The prediction history system replaced this with real data. Every call to `/predict` (outside the cache TTL) logs the prediction to `prediction_history` with the ticker, predicted date, direction, and confidence. A duplicate check prevents re-logging during the 30-minute cache window.

The `fill_actual_outcomes()` function runs during the evening brief job: it queries all `prediction_history` rows with no `actual_direction` yet, fetches current price from yfinance, computes the actual return and direction, marks correct/incorrect, and returns the outcomes for the evening brief message. The `get_accuracy_stats()` function aggregates this into per-ticker accuracy for the weekly digest.

### Frontend V3: Routing and the Landing Page

V3 introduced `react-router-dom` for multi-page navigation. Three routes: `/` (landing page), `/dashboard` (the V2 dashboard), `/hypothesis` (the new research page).

The aesthetic direction for V3 was a deliberate upgrade from V2's generic dark theme. The choice was a terminal/Bloomberg terminal aesthetic: IBM Plex Mono throughout, amber/gold (`#f59e0b`) as the primary accent, near-black backgrounds (`#080808`, `#0a0a0a`), no rounded corners, fine 1px borders, uppercase labels with letter-spacing. The visual language of financial data terminals — grids, data rows, monospace typography — was appropriate for the content.

The animated ticker tape on the landing page used CSS transform/translateX with a JavaScript interval to create a scrolling text effect with live-looking data. The simulated ticker data was honest about being simulated — it reflected actual recent predictions rather than invented numbers.

The `/admin` route was deliberately excluded from the navigation. It's a hidden URL — you have to know `localhost:5173/admin` to access it. The password gate uses `import.meta.env.VITE_ADMIN_PASSWORD` (Vite's env var pattern) stored in a separate `app/frontend/.env` file. The admin password check is client-side (appropriate for a single-user personal tool on localhost) using `sessionStorage` for persistence within a browser session.

---

## Part 6: Key Bugs and Failures in Depth

### Bug 1: The Hardcoded "KO" Ticker
**Version**: V2  
**Impact**: All multi-ticker models trained on wrong data  
**Time lost**: Several hours  
**Root cause**: `fetch_recent_prices(ticker: str = "KO")` — the default parameter was used accidentally when the function was called without its argument in one code path  
**Fix**: Explicit ticker parameter everywhere; logging ticker in every print statement  

### Bug 2: The `parser.py` Name Collision
**Version**: V3  
**Impact**: Silent failure — script ran but produced no output  
**Time lost**: ~30 minutes of confusion  
**Root cause**: Python stdlib has a module called `parser`; naming the hypothesis parser `parser.py` caused the stdlib to be imported instead  
**Fix**: Renamed to `hypothesis_parser.py`  

### Bug 3: Relative Paths Breaking on Import from Subdirectory
**Version**: V3 (occurred three times: `sentiment_loader.py`, `similarity_search.py`, both with `BASE_PATH = Path("../../...")`)  
**Impact**: FileNotFoundError when agents ran from `hypothesis/` subdirectory  
**Time lost**: ~20 minutes each occurrence  
**Root cause**: Relative paths resolve relative to the current working directory, not the file's location  
**Fix**: `Path(__file__).resolve().parent...` in all path definitions  

### Bug 4: torch.load weights_only Breaking Change
**Version**: V2 (discovered when upgrading to PyTorch 2.6)  
**Impact**: All model loading failed with a TypeError  
**Root cause**: PyTorch 2.6 changed default `weights_only=True`; TradeSenpai checkpoints contain non-tensor Python objects  
**Fix**: `torch.load(path, map_location="cpu", weights_only=False)`  

### Bug 5: MA200 Warmup / dropna() Data Loss
**Version**: V2  
**Impact**: Feature DataFrames were shorter than expected; subtle accuracy differences between tickers  
**Root cause**: Computing MA200 requires 200 prior rows; `dropna()` after rolling features removes the warmup rows; if insufficient data fetched, entire feature columns were NaN  
**Fix**: Always fetch `days=800` minimum; verify DataFrame length after `dropna()`  

### Bug 6: groq/compound 413 Error
**Version**: V3 (bear/bull agents)  
**Impact**: Agents 4 and 5 completely non-functional  
**Root cause**: `groq/compound` has a much smaller input token limit than standard models; both the system prompt and query exceeded it  
**Fix**: Switched to Tavily for search (returns structured snippets, not full documents) + llama for formatting  

### Bug 7: JSON Truncation in Synthesizer
**Version**: V3  
**Impact**: Research brief JSON was cut mid-string, parse failed, returned raw text  
**Root cause**: `max_tokens=1200` insufficient for the full brief  
**Fix**: Increased to `max_tokens=2000`; added bracket-closing salvage logic in `_parse_json()`  

### Bug 8: Percentage Double-Multiplication in Market Collector
**Version**: V3  
**Impact**: `distance_from_ma20_pct` showed 323% instead of ~3.2%  
**Root cause**: Feature values already stored as percentages (e.g., 3.23); code multiplied by 100 again  
**Fix**: Removed `* 100` multiplications; values used as-is  

### Bug 9: broadcast() Never Called from Scheduler
**Version**: V3  
**Impact**: All approved subscribers never received any alerts  
**Root cause**: Scheduler imported `send_message` (admin-only); `broadcast()` was built but never wired in  
**Fix**: `from alerts.telegram_bot import broadcast as send_message` — one-line alias  

---

## Part 7: Things We Removed, Simplified, or Deliberately Chose Not to Build

### Removed: FinBERT
Replaced entirely by Loughran-McDonald dictionary. Not a simplification — a genuine improvement. Domain-specific interpretable methods outperformed a general-purpose neural network for this specific task.

### Removed: Single Monolithic Model
Replaced by per-ticker models. A single model trained across all tickers would have to learn six different volatility regimes, sector dynamics, and filing patterns simultaneously. Per-ticker models are simpler and more accurate.

### Deliberately Not Built: Price Prediction
The decision to frame the model as binary direction classification (UP/DOWN) rather than price regression was made early and never revisited. Predicting a specific price target would imply a precision the model cannot support and would be misleading to users. Direction with a confidence score is honest.

### Deliberately Not Built: Real-Time Streaming
Price data is fetched fresh from yfinance on each request (with 30-minute caching). A real-time WebSocket feed would have been more impressive but was unnecessary for a daily prediction horizon.

### Deliberately Not Built: User Authentication
The subscriber system uses Telegram username as identity, with admin-controlled approval. No JWT, no OAuth, no user accounts. Appropriate for the current scale.

### Simplified: Database
SQLite instead of PostgreSQL. At current scale (6 tickers, ~100 potential subscribers, daily predictions), SQLite is perfectly sufficient. The overhead of managing a PostgreSQL instance (even a hosted one on Supabase) would add complexity without adding value.

### Simplified: Deployment
Still on localhost. The architecture supports cloud deployment (Railway, Render) without significant changes, but the current scale doesn't require it.

---

## Part 8: Technical Debt Inventory

| Item | Location | Priority | Notes |
|------|----------|----------|-------|
| Hardcoded API key (original) | Was in `explainer.py` | FIXED | Moved to `.env` in V3 |
| Inline CSS everywhere | All React components | Medium | Works but verbose |
| Zero test coverage | Entire codebase | High | No unit tests, no integration tests |
| Duplicate yfinance fetches | `evidence_agent.py` calls `get_latest_feature_row` twice | Low | Minor performance issue |
| Weekly digest hardcoded accuracy | `scheduler.py` | FIXED | Real DB stats now used |
| No earnings calendar feature | `feature_engineer.py` | High | Known since V2, not yet built |
| No persistent prediction logging persistence | `scheduler.py` `_last_predictions` | FIXED | Now uses SQLite |
| Tavily API key in .env but not .gitignored properly | `.env` | Medium | Verify .gitignore covers it |
| `groq/compound` still used in `explainer.py` | `explainer.py` | Low | Works for its use case |
| `bot_listener.py` polling loop in daemon thread | `main.py` lifespan | Low | Works but not gracefully stoppable |
| No input sanitization on hypothesis text | `POST /hypothesis` | Medium | Currently trusts user input |

---

## Part 9: Current State of the Project

As of V3, TradeSenpai is a functional multi-component research platform:

**Backend (FastAPI on port 8000):**
- 6 trained transformer models, preloaded on startup
- 10 REST endpoints covering predictions, price history, sentiment history, model info, explanations, hypothesis analysis, subscriber management
- Background scheduler running morning briefs (9:30 AM ET), evening briefs (4:15 PM ET), weekly digests (Sunday 6 PM ET), and signal watches (every 2 hours)
- SQLite persistence for alerts, subscribers, and prediction history
- Telegram bot listener for auto-approving subscribers

**Data Pipeline (stock-analysis/):**
- 6 tickers with full SEC filing history fetched from EDGAR
- LM sentiment features computed for each filing
- Merged datasets combining price features and sentiment, one CSV per ticker
- Separate model checkpoints per ticker

**Frontend (React/Vite on port 5173):**
- 4 routes: landing page, dashboard, hypothesis engine, admin panel
- Terminal/Bloomberg aesthetic with IBM Plex Mono typography
- Hidden `/admin` route with password gate for subscriber management

**Integrations:**
- yfinance: live price data
- EDGAR API: SEC filings
- Groq (llama-3.3-70b-versatile): LLM synthesis
- Tavily: web search for current news
- Telegram Bot API: alert delivery

**Model performance:**
- KO: 52.29% CV accuracy
- JNJ: 51.97% CV accuracy
- PG: 51.92% CV accuracy
- WMT: 53.33% CV accuracy
- AAPL: 52.40% CV accuracy
- GOOGL: 53.48% CV accuracy

---

## Part 10: What We Would Do Differently If Rebuilding Today

**1. Start with proper path management from day one.**
Every path-related bug — and there were at least three significant ones — came from using relative paths. The fix (`Path(__file__).resolve().parent...`) is not complicated. It should be the default from the first line of code.

**2. Build a test suite in V1.**
No tests were written for any version. The test coverage is zero. For a project where feature consistency between training and inference is critical, this is a significant risk. A test that verifies `feature_engineer.py` produces the same 56 features in the same order as the training pipeline would have caught several bugs earlier.

**3. Use environment variables for everything from day one.**
The Groq API key was hardcoded in V1/V2. Moving to `.env` should have been the starting point, not a cleanup task.

**4. Define the feature schema once and enforce it.**
The 56-feature set is defined implicitly across `feature_engineer.py`, the model training scripts, and the inference pipeline. A single canonical list with a validation function that checks both training and inference against it would prevent the class of bugs where a feature is added to training but not to inference (or vice versa).

**5. Name files carefully from the start.**
The `parser.py` → `hypothesis_parser.py` rename cost 30 minutes. File names should be checked against Python stdlib module names before committing.

**6. Start with `broadcast()`, not `send_message()`.**
The subscriber system was built but not actually connected to the scheduler for most of V3 development. Building `broadcast()` first and making it the only send primitive would have avoided this oversight.

**7. Document the data schema early.**
The merged dataset CSV columns, the feature columns in the model checkpoint, and the API response schemas evolved organically. A schema file written at the start of V2 would have made multi-agent development in V3 much cleaner.

---

## Part 11: The Biggest Lessons

**Lesson 1: Domain knowledge beats scale.**
Loughran-McDonald beating FinBERT is the central technical lesson of this entire project. A curated 2,700-word dictionary built by two finance professors outperformed a 110-million-parameter neural network for financial document analysis. The reason is domain fit. When your domain is specific enough, a simple tool built for that domain will outperform a powerful tool built for a different one.

**Lesson 2: Honest accuracy metrics are more valuable than impressive ones.**
52% CV accuracy sounds unimpressive. But it is honest, reproducible, and appropriate for the task. Chasing a higher number would have meant overfitting, data leakage, or misleading methodology. The decision to document and display the actual accuracy — and to explain why it is meaningful — was a better intellectual position than trying to inflate the number.

**Lesson 3: Silent failures are the hardest bugs.**
The hardcoded "KO" bug, the `parser.py` naming collision, and the `send_message` vs `broadcast` oversight all shared the same property: the code ran without errors. Nothing crashed. Nothing threw an exception. The system appeared to work while doing the wrong thing. Logging, assertions, and explicit validation at key points are the only defenses against this class of bug.

**Lesson 4: Reuse aggressively but carefully.**
V3's agent architecture achieved a lot of leverage by reusing V2's infrastructure: `feature_engineer.py`, `similarity_search.py`, `sentiment_loader.py`, `predictor.py`. But reuse also meant that a path bug in `similarity_search.py` broke three different agents simultaneously. Shared infrastructure is powerful; it also means shared failure modes.

**Lesson 5: The path from prototype to system is mostly about the boring stuff.**
The transformer architecture, the LM sentiment pipeline, the hypothesis synthesis — these are the interesting parts. But the actual hours of development were spent on: path resolution bugs, PyTorch version compatibility, JSON parsing edge cases, Telegram message formatting, SQLite schema design, APScheduler timezone configuration, and API authentication. The boring infrastructure work is what makes the interesting parts actually run.

**Lesson 6: Name things for what they do, not what they are.**
`parser.py` conflicted with stdlib because it described the type of thing (a parser) rather than its specific role (hypothesis parser). `sentiment_loader.py` is clear; `utils.py` would not be. Specific, role-describing file names prevent both naming collisions and ambiguity about where to find things.

**Lesson 7: Build the hardcoded version before the parameterized version.**
The multi-ticker bug originated because the parameterization of the ticker argument was done inconsistently. The pattern that worked: build the single-ticker version fully, verify it, then add the ticker parameter to everything systematically. Partial parameterization (where some functions take ticker as an argument and some use the default) is the failure mode.

**Lesson 8: The disclaimer is not just legal cover — it's design philosophy.**
"Educational simulation only. Not financial advice. Model accuracy ~52%." This disclaimer appears in every alert, every brief, every API response, and every frontend component. It's not there to limit liability — it's there because it's true, and because a tool that was honest about its limitations was more useful than one that wasn't. Users who understand what the tool is — a research augmentation system with explicit uncertainty bounds — use it better than users who think it's a prediction engine.

---

## Conclusion

TradeSenpai went from a single-ticker sentiment classifier to a multi-component research platform with a hypothesis engine, scheduled alerts, subscriber management, and a production-quality frontend. It works. It's honest about what it does and doesn't do. The technical choices — LM dictionary over FinBERT, per-ticker transformers, SQLite over PostgreSQL, inline CSS over frameworks, explicit accuracy reporting — were made deliberately and held up under development pressure.

Version 4 has a clear roadmap: cloud deployment, earnings calendar feature integration, prediction history visualization in the dashboard, more tickers, and potentially a backtesting view. The foundation built across V1-V3 is solid enough to support all of it.

The most important thing TradeSenpai demonstrates is not the technology — it's the methodology. Scope ruthlessly. Go deep not wide. Domain-specific beats general-purpose. Honest beats impressive. These principles were tested repeatedly across three versions and held every time.

---

*Document generated at end of V3 development. Next version: V4.*  
*Total versions: 3 | Total tickers: 6 | Total endpoints: 10 | Total agents: 6 | Total lessons learned: too many to count.*